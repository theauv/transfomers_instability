preprocessing_num_workers: null
per_device_eval_batch_size: 32  # 32
per_device_train_batch_size: 32  # 32
weight_decay: 0.0 #0.0 | 0.001
learning_rate: 1.e-2 #1.e-2 | 2e-3
gradient_accumulation_steps: 4 #4
num_train_epochs: 1
max_train_steps: null
lr_scheduler_type: cosine #linear
num_warmup_steps: 300  # 5e3
mixed_precision: "no" #fp16
beta_1: 0.9
beta_2: 0.99999