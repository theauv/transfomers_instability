preprocessing_num_workers: null
per_device_eval_batch_size: 16 # 32
per_device_train_batch_size: 16  # 32
weight_decay: 0.0 #0.0 | 0.001
learning_rate: 2.e-2 #1.e-2 | 2e-3 | 5e-3 | 8e-3
gradient_accumulation_steps: 4 #4
num_train_epochs: 10
max_train_steps: null
lr_scheduler_type: cosine #linear
num_warmup_steps: 300  # 5e3
mixed_precision: "no" #fp16
beta_1: 0.9
beta_2: 0.99999