preprocessing_num_workers: null
per_device_eval_batch_size: 32  # 32
per_device_train_batch_size: 32  # 32
weight_decay: 0.001 #0.0
learning_rate: 2.e-3 #1.e-2
gradient_accumulation_steps: 1 #4
num_train_epochs: 1
max_train_steps: null
lr_scheduler_type: cosine #linear
num_warmup_steps: 300  # 5e3
mixed_precision: "no" #fp16